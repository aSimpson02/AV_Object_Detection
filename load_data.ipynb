{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries:::\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: paths to the dataset in downloads folder\n",
    "path = 'KittiDataset'\n",
    "train_image_path = os.path.join(path, 'image_2', 'training')\n",
    "train_label_path = os.path.join(path, 'label_2')\n",
    "train_calib_path = os.path.join(path, 'calib', 'training')\n",
    "\n",
    "test_image_path = os.path.join(path, 'image_2', 'testing')\n",
    "test_calib_path = os.path.join(path, 'calib', 'testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified path: KittiDataset/image_2/training\n",
      "Verified path: KittiDataset/label_2\n",
      "Verified path: KittiDataset/calib/training\n",
      "Verified path: KittiDataset/image_2/testing\n",
      "Verified path: KittiDataset/calib/testing\n"
     ]
    }
   ],
   "source": [
    "# Verify Dataset Paths\n",
    "def verify_paths(paths):\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Path does not exist: {p}\")\n",
    "        else:\n",
    "            print(f\"Verified path: {p}\")\n",
    "\n",
    "paths = [train_image_path, train_label_path, train_calib_path, test_image_path, test_calib_path]\n",
    "verify_paths(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing:::\n",
    "#add in data here, augmentation here too\n",
    "def data_preprocessing(image_path, label_path=None):\n",
    "    print(f\"Processing images in: {image_path}\")\n",
    "    if not isinstance(image_path, str):\n",
    "        raise TypeError(f\"Expected a string for image_path, but got {type(image_path)}\")\n",
    "\n",
    "    images, labels = [], []\n",
    "\n",
    "    #lopping through to find all images in folders \n",
    "    for file_name in os.listdir(image_path):\n",
    "        #print(f\"Contents of {image}: {os.listdir(image)}\")\n",
    "        if file_name.lower().endswith('.png'):\n",
    "            #loading\n",
    "            img_file = os.path.join(image_path, file_name)\n",
    "            img = cv2.imread(img_file)\n",
    "            if img is None:\n",
    "                print(f\"Warning: Unable to load image: {img_file}\")\n",
    "                continue\n",
    "            #resizing and normalizing data on image\n",
    "            img = cv2.resize(img, (64, 64))\n",
    "            img = img / 255.0 \n",
    "            images.append(img)\n",
    "\n",
    "\n",
    "\n",
    "    #lopping through to find all labels in folders\n",
    "        if label_path:\n",
    "                label_file = os.path.join(label_path, file_name.replace('.png', '.txt'))\n",
    "                if os.path.exists(label_file):\n",
    "                    with open(label_file, 'r') as f:\n",
    "                        labels.append(f.read().strip())\n",
    "                else:\n",
    "                    labels.append(None)\n",
    "\n",
    "    return np.array(images), labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n",
      "Processing images in: KittiDataset/image_2/training\n",
      "Loading and preprocessing testing data...\n",
      "Processing images in: KittiDataset/image_2/testing\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/000797.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001891.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001271.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/002752.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001307.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/004252.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/004397.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/003229.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001600.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001832.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001013.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/000889.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/003234.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/004166.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001976.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/004467.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/000819.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001119.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/004514.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/001518.png\n",
      "Warning: Unable to load image: KittiDataset/image_2/testing/003680.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng error: IDAT: CRC error\n"
     ]
    }
   ],
   "source": [
    "#loading test and training data\n",
    "print(\"Loading and preprocessing training data...\")\n",
    "train_images, train_labels = data_preprocessing(train_image_path, train_label_path)\n",
    "\n",
    "print(\"Loading and preprocessing testing data...\")\n",
    "test_images, _ = data_preprocessing(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels...\n"
     ]
    }
   ],
   "source": [
    "# Encode Labels\n",
    "if train_labels:\n",
    "    print(\"Encoding labels...\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform([label for label in train_labels if label is not None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data augmentation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (21, 64, 64, 3), y.shape = (7,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datagen\u001b[38;5;241m.\u001b[39mflow(X_train, y_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying data augmentation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mdata_augmentation\u001b[0;34m(X_train, y_train, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m      5\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      6\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     fill_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#train_data = datagen.flow(X_train, y_train, batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#return train_data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1545\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1488\u001b[0m     x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1499\u001b[0m ):\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes data & label arrays, generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \n\u001b[1;32m   1544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumpyArrayIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/keras/src/preprocessing/image.py:707\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[1;32m    704\u001b[0m     x_misc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, y.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    712\u001b[0m     )\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_weight):\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, sample_weight.shape = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39masarray(sample_weight)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    719\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (21, 64, 64, 3), y.shape = (7,)"
     ]
    }
   ],
   "source": [
    "#DATA AUGMENTATION\n",
    "# Data Augmentation for training data, not validation\n",
    "def data_augmentation(X_train, y_train, batch_size=32):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    #train_data = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "    #return train_data\n",
    "    return datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "print(\"Applying data augmentation...\")\n",
    "train_data = data_augmentation(train_images, encoded_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data augmentation...\n"
     ]
    }
   ],
   "source": [
    "#calling functions above in order:::\n",
    "#data_preprocessed = data_preprocessing(data)\n",
    "\n",
    "print(\"Applying data augmentation...\")\n",
    "train_data = data_augmentation(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data preprocessing complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "# #saving data\n",
    "print(\"Saving data...\")\n",
    "np.save('train_images.npy', train_images)\n",
    "np.save('train_labels.npy', encoded_labels)\n",
    "np.save('test_images.npy', test_images)\n",
    "#np.save('test_calib.npy', test_calib)\n",
    "\n",
    "print(\"Data preprocessing complete. Files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
